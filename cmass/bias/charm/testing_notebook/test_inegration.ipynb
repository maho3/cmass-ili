{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import torch\n",
    "dev = torch.device(\"cuda\")\n",
    "import torch.optim as optim\n",
    "# root_dir = '/mnt/home/spandey/ceph/ltu-cmass/cmass/bias/charm/'\n",
    "# os.chdir(root_dir)\n",
    "import sys, os\n",
    "# sys.path.append('/mnt/home/spandey/ceph/ltu-cmass/cmass/bias/charm')\n",
    "os.chdir('/mnt/home/spandey/ceph/ltu-cmass/cmass/bias/charm')\n",
    "from combined_models import COMBINED_Model\n",
    "from all_models import *\n",
    "from utils_data_prep_cosmo import *\n",
    "from colossus.cosmology import cosmology\n",
    "params = {'flat': True, 'H0': 67.11, 'Om0': 0.3175, 'Ob0': 0.049, 'sigma8': 0.834, 'ns': 0.9624}\n",
    "cosmo = cosmology.setCosmology('myCosmo', **params)\n",
    "# get halo mass function:\n",
    "from colossus.lss import mass_function\n",
    "from tqdm import tqdm\n",
    "    \n",
    "import yaml\n",
    "import pickle as pk\n",
    "# autoreload modules\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pl\n",
    "import os  # noqa\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'  # noqa, must go before jax\n",
    "\n",
    "import numpy as np\n",
    "# import logging\n",
    "# import hydra\n",
    "from copy import deepcopy\n",
    "# from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "from os.path import join as pjoin\n",
    "from scipy.integrate import quad\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline as IUS\n",
    "# from .tools.halo_models import TruncatedPowerLaw\n",
    "# from .tools.halo_sampling import (pad_3d, sample_3d,\n",
    "#                                   sample_velocities_density,\n",
    "#                                   sample_velocities_kNN,\n",
    "#                                   sample_velocities_CIC)\n",
    "# from ..utils import get_source_path, timing_decorator, load_params\n",
    "\n",
    "\n",
    "def parse_config(cfg):\n",
    "    with open_dict(cfg):\n",
    "        cfg.nbody.cosmo = load_params(cfg.nbody.lhid, cfg.meta.cosmofile)\n",
    "    return cfg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_model_interface:\n",
    "\n",
    "    def __init__(self, run_config_name):\n",
    "        with open(\"configs/\" + run_config_name,\"r\") as file_object:\n",
    "            config=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "\n",
    "\n",
    "        config_sims = config['sim_settings']\n",
    "        ji_array = np.arange(int(config_sims['nsims']))\n",
    "        num_cosmo_params = int(config_sims['num_cosmo_params'])\n",
    "        ns_d = config_sims['ns_d']\n",
    "        nb = config_sims['nb']\n",
    "        nf = config_sims['nf']\n",
    "        self.nf = nf\n",
    "        layers_types = config_sims['layers_types']\n",
    "        z_inference = config_sims['z_inference']\n",
    "        nc = 0\n",
    "        for jl in range(len(layers_types)):\n",
    "            if layers_types[jl] == 'cnn':\n",
    "                nc += 1\n",
    "            elif layers_types[jl] == 'res':\n",
    "                nc += 2\n",
    "            else:\n",
    "                raise ValueError(\"layer type not supported\")\n",
    "        self.nc = nc\n",
    "\n",
    "        z_all = config_sims['z_all']\n",
    "        z_all_FP = config_sims['z_all_FP']\n",
    "        self.z_all_FP = z_all_FP\n",
    "        ns_h = config_sims['ns_h']\n",
    "        self.ns_h = ns_h\n",
    "        nax_h = ns_h // nb\n",
    "        self.nax_h = nax_h\n",
    "        cond_sim = config_sims['cond_sim']\n",
    "\n",
    "        mass_type = config_sims['mass_type']\n",
    "        lgMmin = config_sims['lgMmin']\n",
    "        lgMmax = config_sims['lgMmax']\n",
    "        self.lgMmin = lgMmin\n",
    "        self.lgMmax = lgMmax\n",
    "        stype = config_sims['stype']\n",
    "        rescale_sub = config_sims['rescale_sub']\n",
    "\n",
    "        try:\n",
    "            Nmax = config_sims['Nmax']\n",
    "        except:\n",
    "            Nmax = 4\n",
    "\n",
    "        config_net = config['network_settings']\n",
    "        hidden_dim_MAF = config_net['hidden_dim_MAF']\n",
    "        learning_rate = config_net['learning_rate']\n",
    "        K_M1 = config_net['K_M1']\n",
    "        B_M1 = config_net['B_M1']\n",
    "        nflows_M1_NSF = config_net['nflows_M1_NSF']\n",
    "\n",
    "        K_Mdiff = config_net['K_Mdiff']\n",
    "        B_Mdiff = config_net['B_Mdiff']\n",
    "        nflows_Mdiff_NSF = config_net['nflows_Mdiff_NSF']\n",
    "\n",
    "        base_dist_Ntot = config_net['base_dist_Ntot']\n",
    "        if base_dist_Ntot == 'None':\n",
    "            base_dist_Ntot = None\n",
    "        base_dist_M1 = config_net['base_dist_M1']\n",
    "        base_dist_Mdiff = config_net['base_dist_Mdiff']\n",
    "        ngauss_M1 = config_net['ngauss_M1']\n",
    "\n",
    "        ksize = nf\n",
    "        nfeature_cnn = config_net['nfeature_cnn']\n",
    "        nout_cnn = 4 * nfeature_cnn\n",
    "        if cond_sim == 'fastpm':\n",
    "            ninp = len(z_all_FP)\n",
    "        elif cond_sim == 'quijote':\n",
    "            ninp = len(z_all)\n",
    "        else:\n",
    "            raise ValueError(\"cond_sim not supported\")\n",
    "\n",
    "        num_cond = nout_cnn + ninp + num_cosmo_params\n",
    "\n",
    "\n",
    "        lgM_array = np.linspace(lgMmin, lgMmax, 1000)\n",
    "        M_array = 10**lgM_array\n",
    "        if '200c' in mass_type:\n",
    "            hmf = mass_function.massFunction(M_array, float(z_inference), mdef = '200c', model = 'tinker08', q_out = 'dndlnM')\n",
    "        if 'vir' in mass_type:\n",
    "            hmf = mass_function.massFunction(M_array, float(z_inference), mdef = 'vir', model = 'tinker08', q_out = 'dndlnM')    \n",
    "        if 'fof' in mass_type:\n",
    "            hmf = mass_function.massFunction(M_array, float(z_inference), mdef = 'fof', model = 'bhattacharya11', q_out = 'dndlnM')\n",
    "        lgM_rescaled = rescale_sub + (lgM_array - lgMmin)/(lgMmax-lgMmin)\n",
    "\n",
    "        int_val = sp.integrate.simps(hmf, lgM_rescaled)\n",
    "        hmf_pdf = hmf/int_val\n",
    "        # define the cdf of the halo mass function\n",
    "        hmf_cdf = np.zeros_like(hmf_pdf)\n",
    "        for i in range(len(hmf_cdf)):\n",
    "            hmf_cdf[i] = sp.integrate.simps(hmf_pdf[:i+1], lgM_rescaled[:i+1])\n",
    "\n",
    "        ndim_diff =  Nmax - 1\n",
    "        self.ndim_diff = ndim_diff\n",
    "\n",
    "        with open(\"/mnt/home/spandey/ceph/AR_NPE/run_configs/CMASS_test/\" + run_config_name,\"r\") as file_object:\n",
    "            config=yaml.load(file_object,Loader=yaml.SafeLoader)\n",
    "\n",
    "        config_train = config['train_settings']\n",
    "\n",
    "        save_string = config_train['save_string']\n",
    "\n",
    "        save_bestfit_model_dir = '/mnt/home/spandey/ceph/AR_NPE/' + \\\n",
    "                                'TEST_VARY_COSMO/HRES_SUMGAUSS_subsel_random_MULT_GPU_NO_VELOCITY_ns_' + \\\n",
    "                                    str(len(ji_array)) + \\\n",
    "                                    '_cond_sim_' + cond_sim  + '_ns_' + str(ns_h) \\\n",
    "                                    + '_nc' + str(nc) + '_mass_' + mass_type + \\\n",
    "                                    '_KM1_' + str(K_M1) + \\\n",
    "                                    '_stype_' + stype + \\\n",
    "                                    '_Nmax' + str(Nmax) + save_string\n",
    "\n",
    "        if 'sigv' in config_net:\n",
    "            sigv = config_net['sigv']\n",
    "        else:\n",
    "            sigv = 0.05\n",
    "        mu_all = np.arange(Nmax + 1) + 1\n",
    "        sig_all = sigv * np.ones_like(mu_all)\n",
    "        ngauss_Nhalo = Nmax + 1\n",
    "\n",
    "\n",
    "        num_cond_Ntot = num_cond\n",
    "\n",
    "        model_BinaryMask = SumGaussModel(\n",
    "            hidden_dim=hidden_dim_MAF,\n",
    "            num_cond=num_cond_Ntot,\n",
    "            ngauss=2,\n",
    "            mu_all=mu_all[:2],\n",
    "            sig_all=sig_all[:2],\n",
    "            base_dist=base_dist_Ntot   \n",
    "            )\n",
    "\n",
    "        model_BinaryMask.to(dev)\n",
    "\n",
    "\n",
    "        model_multiclass = SumGaussModel(\n",
    "            hidden_dim=hidden_dim_MAF,\n",
    "            num_cond=num_cond_Ntot,\n",
    "            ngauss=ngauss_Nhalo - 1,\n",
    "            mu_all=mu_all[1:] - 1,\n",
    "            sig_all=sig_all[1:],\n",
    "            base_dist=base_dist_Ntot   \n",
    "            )\n",
    "\n",
    "\n",
    "        model_multiclass.to(dev)\n",
    "\n",
    "        num_cond_M1 = num_cond + 1\n",
    "\n",
    "        model_M1 = NSF_M1_CNNcond(\n",
    "            K=K_M1,\n",
    "            B=B_M1,\n",
    "            hidden_dim=hidden_dim_MAF,\n",
    "            num_cond=num_cond_M1,\n",
    "            nflows=nflows_M1_NSF,\n",
    "            base_dist=base_dist_M1,\n",
    "            ngauss=ngauss_M1,\n",
    "            lgM_rs_tointerp=lgM_rescaled,\n",
    "            hmf_pdf_tointerp=hmf_pdf,\n",
    "            hmf_cdf_tointerp=hmf_cdf    \n",
    "            )\n",
    "\n",
    "        num_cond_Mdiff = num_cond + 2\n",
    "        model_Mdiff = NSF_Mdiff_CNNcond(\n",
    "            dim=ndim_diff,\n",
    "            K=K_Mdiff,\n",
    "            B=B_Mdiff,\n",
    "            hidden_dim=hidden_dim_MAF,\n",
    "            num_cond=num_cond_Mdiff,\n",
    "            nflows=nflows_Mdiff_NSF,\n",
    "            base_dist=base_dist_Mdiff,\n",
    "            mu_pos=True\n",
    "            )\n",
    "\n",
    "        ndim = ndim_diff + 1\n",
    "        model = COMBINED_Model(\n",
    "            None,\n",
    "            model_Mdiff,\n",
    "            model_M1,\n",
    "            model_BinaryMask,\n",
    "            model_multiclass,\n",
    "            ndim,\n",
    "            ksize,\n",
    "            ns_d,\n",
    "            ns_h,\n",
    "            1,    \n",
    "            ninp,\n",
    "            nfeature_cnn,\n",
    "            nout_cnn,\n",
    "            layers_types=layers_types,\n",
    "            act='tanh',\n",
    "            padding='valid',\n",
    "            sep_Binary_cond=True,\n",
    "            sep_MultiClass_cond=True,\n",
    "            sep_M1_cond=True,\n",
    "            sep_Mdiff_cond=True,\n",
    "            num_cond_Binary = num_cond_Ntot,\n",
    "            num_cond_MultiClass = num_cond_Ntot,\n",
    "            num_cond_M1 = num_cond_M1,\n",
    "            num_cond_Mdiff = num_cond_Mdiff\n",
    "            )\n",
    "\n",
    "        model= torch.nn.DataParallel(model)\n",
    "        model.to(dev)\n",
    "        print()\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        loss_min = 1e20\n",
    "        epoch_tot_counter = 0\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.25, patience=1000, verbose=True, cooldown=1000, min_lr=1e-5)\n",
    "\n",
    "        jf = 1\n",
    "        save_bestfit_model_name = save_bestfit_model_dir + '/flow_' + str(jf)\n",
    "\n",
    "        print('loading bestfit model')\n",
    "        bestfit_model = (torch.load(save_bestfit_model_name))\n",
    "        model.load_state_dict(bestfit_model['state_dict'])\n",
    "        optimizer.load_state_dict(bestfit_model['optimizer'])\n",
    "        scheduler.load_state_dict(bestfit_model['scheduler'])\n",
    "        loss_min = bestfit_model['loss_min']\n",
    "        loss = bestfit_model['loss']\n",
    "        # lr = bestfit_model['lr']\n",
    "        epoch_tot_counter = bestfit_model['epoch_tot_counter']\n",
    "        self.model = model\n",
    "        print(loss_min, epoch_tot_counter)\n",
    "\n",
    "\n",
    "    def process_input_density(self, test_LH_id, rho_m_zg=None, rho_m_zIC=None):\n",
    "        n_dim_red = (self.nf - 1) // 2\n",
    "        n_pad = n_dim_red * self.nc\n",
    "\n",
    "        if rho_m_zg is None:\n",
    "            df_zg = pk.load(open('/mnt/ceph/users/spandey/Quijote/data_NGP_self_fastpm_LH/%d/density_HR_full_m_res_128_z=0.5_nbatch_8_nfilter_3_ncnn_0.pk'%test_LH_id,'rb'))\n",
    "            df_test_zg = df_zg['density_cic_unpad_combined']\n",
    "        else:\n",
    "            df_test_zg = rho_m_zg\n",
    "        df_test_pad_zg = np.pad(df_test_zg, n_pad, 'wrap')\n",
    "\n",
    "        if rho_m_zIC is None:\n",
    "            df_zIC = pk.load(open('/mnt/ceph/users/spandey/Quijote/data_NGP_self_fastpm_LH/%d/density_HR_full_m_res_128_z=99_nbatch_8_nfilter_3_ncnn_0.pk'%test_LH_id,'rb'))\n",
    "            df_test_zIC = df_zIC['density_cic_unpad_combined']\n",
    "        else:\n",
    "            df_test_zIC = rho_m_zIC\n",
    "\n",
    "        df_test_pad_zIC = np.pad(df_test_zIC, n_pad, 'wrap')\n",
    "\n",
    "        z_REDSHIFT_diff_sig_VALUE = self.z_all_FP[-1]\n",
    "        VALUE_SIG = float(z_REDSHIFT_diff_sig_VALUE.split('_')[4])\n",
    "        density_smoothed = gaussian_filter(df_test_pad_zg, sigma=VALUE_SIG)\n",
    "        df_test_pad_constrast_zg = density_smoothed - df_test_pad_zg\n",
    "\n",
    "        df_test_all_pad = np.stack([np.log(1 + df_test_pad_zg + 1e-10), np.log(1 + df_test_pad_zIC+ 1e-10), df_test_pad_constrast_zg], axis=0)[None,None,:]\n",
    "\n",
    "\n",
    "        density_smoothed = gaussian_filter(df_test_zg, sigma=VALUE_SIG)\n",
    "        df_test_constrast_zg = density_smoothed - df_test_zg\n",
    "\n",
    "        df_test_all_unpad = np.stack([np.log(1 + df_test_zg + 1e-10), np.log(1 + df_test_zIC + 1e-10), df_test_constrast_zg], axis=0)[None,None,:]\n",
    "\n",
    "        cond_nsh_test = np.moveaxis(df_test_all_unpad, 2, 5)\n",
    "        nsims_test = cond_nsh_test.shape[1]\n",
    "        nax_h_test = cond_nsh_test.shape[2]\n",
    "        ninp_test = cond_nsh_test.shape[-1]\n",
    "        cond_tensor_nsh_test = torch.Tensor(np.copy(cond_nsh_test.reshape(1,nsims_test * (nax_h_test ** 3), ninp_test))).cuda(dev)    \n",
    "\n",
    "        LH_cosmo_val_file='/mnt/home/spandey/ceph/Quijote/latin_hypercube_params.txt'\n",
    "        LH_cosmo_val_all = np.loadtxt(LH_cosmo_val_file)\n",
    "        # cosmo_val_test = np.tile(LH_cosmo_val_all[test_LH_id], (*df_test_all_unpad.shape ,1))[0,...][None,:]\n",
    "        cosmo_val_test = np.tile(LH_cosmo_val_all[test_LH_id], (cond_tensor_nsh_test.shape[1] ,1))[None,:]\n",
    "\n",
    "        # df_test_all_pad.shape, df_test_all_unpad.shape, cosmo_val_test.shape\n",
    "        df_test_all_pad = torch.tensor(df_test_all_pad).to(dev)\n",
    "        df_test_all_unpad = torch.tensor(cond_tensor_nsh_test).to(dev)\n",
    "        cosmo_val_test = torch.tensor(cosmo_val_test, dtype=torch.float32).to(dev)\n",
    "\n",
    "        train_Ntot, train_M1, train_Mdiff = 1, 1, 1\n",
    "        train_binary, train_multi = 1, 1\n",
    "        Ntot_samp_test, M1_samp_test, M_diff_samp_test, mask_tensor_M1_samp_test, mask_tensor_Mdiff_samp_test, _ = self.model.module.inverse(\n",
    "            cond_x=df_test_all_pad,\n",
    "            cond_x_nsh=df_test_all_unpad,\n",
    "            cond_cosmo=cosmo_val_test,\n",
    "            use_truth_Nhalo=1-train_Ntot,\n",
    "                use_truth_M1=1-train_M1,\n",
    "                use_truth_Mdiff=1-train_Mdiff, \n",
    "            mask_Mdiff_truth=None,\n",
    "            mask_M1_truth=None,\n",
    "            Nhalos_truth=None,\n",
    "            M1_truth=None,\n",
    "            Mdiff_truth=None,\n",
    "            train_binary=train_binary,\n",
    "            train_multi=train_multi,   \n",
    "            train_M1=train_M1,\n",
    "            train_Mdiff=train_Mdiff,\n",
    "            )\n",
    "\n",
    "        Ntot_samp_test = Ntot_samp_test[0][:,np.newaxis]\n",
    "        save_subvol_Nhalo = Ntot_samp_test.reshape(nsims_test, nax_h_test, nax_h_test, nax_h_test)\n",
    "        save_subvol_M1 = (M1_samp_test[0] * mask_tensor_M1_samp_test[0][:,0]\n",
    "                            ).cpu().detach().numpy().reshape(nsims_test, nax_h_test, nax_h_test, nax_h_test, 1)\n",
    "        save_subvol_Mdiff = (M_diff_samp_test[0] * mask_tensor_Mdiff_samp_test[0]\n",
    "                                ).cpu().detach().numpy().reshape(nsims_test, nax_h_test, nax_h_test, nax_h_test, self.ndim_diff)\n",
    "\n",
    "        mask_subvol_Mtot1 = mask_tensor_M1_samp_test[0].cpu().detach().numpy().reshape(nsims_test, nax_h_test, nax_h_test, nax_h_test)[...,None]\n",
    "        mask_subvol_Mtot2 = mask_tensor_Mdiff_samp_test[0].cpu().detach().numpy().reshape(nsims_test, nax_h_test, nax_h_test, nax_h_test, self.ndim_diff)\n",
    "        mask_subvol_Mtot = np.concatenate([mask_subvol_Mtot1, mask_subvol_Mtot2], axis=-1)\n",
    "\n",
    "        save_subvol_Mtot = np.zeros((nsims_test, nax_h_test, nax_h_test, nax_h_test, self.ndim_diff + 1))\n",
    "        # Mmin, Mmax = return_dict_test['Mmin'], return_dict_test['Mmax']\n",
    "        for jd in range(self.ndim_diff + 1):\n",
    "            if jd == 0:\n",
    "                save_subvol_Mtot[..., jd] = (save_subvol_M1[..., 0] + 0.5) * (self.lgMmax - self.lgMmin) + self.lgMmin\n",
    "            else:\n",
    "                save_subvol_Mtot[...,\n",
    "                                jd] = (save_subvol_Mtot[..., jd - 1]) - (save_subvol_Mdiff[..., jd - 1]) * (self.lgMmax - self.lgMmin)\n",
    "\n",
    "\n",
    "        save_subvol_Mtot *= mask_subvol_Mtot\n",
    "\n",
    "        Nhalos = save_subvol_Nhalo[0,...]\n",
    "        M_halos = save_subvol_Mtot[0,...]\n",
    "                    \n",
    "        # create the meshgrid\n",
    "        xall = (np.linspace(0, 1000, self.ns_h + 1))\n",
    "        xarray = 0.5 * (xall[1:] + xall[:-1])\n",
    "        yarray = np.copy(xarray)\n",
    "        zarray = np.copy(xarray)\n",
    "        x_cy, y_cy, z_cy = np.meshgrid(xarray, yarray, zarray, indexing='ij')\n",
    "\n",
    "\n",
    "        x_h_mock, y_h_mock, z_h_mock, lgM_mock = [], [], [], []\n",
    "        # Nmax_sel = 3\n",
    "        k = 0\n",
    "        for jx in range(self.ns_h):\n",
    "            for jy in range(self.ns_h):\n",
    "                for jz in range(self.ns_h):\n",
    "                        Nh_vox = int(Nhalos[jx, jy, jz])\n",
    "                        if Nh_vox > 0:\n",
    "                            x_h_mock.append(x_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                            y_h_mock.append(y_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                            z_h_mock.append(z_cy[jx, jy, jz]*np.ones(Nh_vox))\n",
    "                            \n",
    "                            lgM_mock.append((M_halos[jx, jy, jz, :Nh_vox]))\n",
    "                            k += Nh_vox\n",
    "\n",
    "        # convert to numpy arrays\n",
    "        x_h_mock = np.concatenate(x_h_mock)\n",
    "        y_h_mock = np.concatenate(y_h_mock)\n",
    "        z_h_mock = np.concatenate(z_h_mock)\n",
    "        pos_h_mock = np.vstack((x_h_mock, y_h_mock, z_h_mock)).T\n",
    "        lgMass_mock = np.concatenate(lgM_mock)\n",
    "        # convert to float data type\n",
    "        pos_h_mock = pos_h_mock.astype('float32')\n",
    "        lgMass_mock = lgMass_mock.astype('float32')\n",
    "\n",
    "        return pos_h_mock, lgMass_mock\n",
    "                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/users/spandey/ltu-cmass/cmass/bias/charm\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/spandey/ceph/env/ltu_cmass/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bestfit model\n",
      "-34.86669921875 20278\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import torch\n",
    "dev = torch.device(\"cuda\")\n",
    "import torch.optim as optim\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# root_dir = '/mnt/home/spandey/ceph/ltu-cmass/cmass/bias/charm/'\n",
    "# os.chdir(root_dir)\n",
    "import sys, os\n",
    "sys.path.append('/mnt/home/spandey/ceph/ltu-cmass/cmass/bias/charm')\n",
    "from integrate_ltu_cmass import *\n",
    "\n",
    "run_config_name = 'MULTGPU_cond_fastpm_ns128_run_Ntot_M1_Mdiff_subselrand_gumbel.yaml'\n",
    "charm_interface = get_model_interface(run_config_name)    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded density at zg=0.5 with shape (136, 136, 136)\n",
      "loaded density at IC zIC=99 with shape (136, 136, 136)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/spandey/ceph/ltu-cmass/cmass/bias/charm/integrate_ltu_cmass.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  df_test_all_unpad = torch.tensor(cond_tensor_nsh_test).to(dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the model\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_LH_id = 0\n",
    "pos_h_mock, lgMass_mock = charm_interface.process_input_density(test_LH_id=test_LH_id, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ili-sbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
